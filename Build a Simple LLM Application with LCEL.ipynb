{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e4c46ce",
   "metadata": {},
   "source": [
    "https://python.langchain.com/v0.2/docs/tutorials/llm_chain/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4f7ca0",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2fbd846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from langsmith import traceable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c248a5",
   "metadata": {},
   "source": [
    "## Setup Langsmith for improved debug capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6db8c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c15c6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c188b47",
   "metadata": {},
   "source": [
    "## Connect to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd1bca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1232897b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "966c6ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "?ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcce369",
   "metadata": {},
   "source": [
    "## Directly pass messages to the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e173f9a1",
   "metadata": {},
   "source": [
    "This is not the usual approach, but we start with this to demonstrate how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2383484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao!', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 20, 'total_tokens': 23}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d576307f90', 'finish_reason': 'stop', 'logprobs': None}, id='run-870f01d4-3a09-4f9d-bc9c-f91d6920c0b8-0', usage_metadata={'input_tokens': 20, 'output_tokens': 3, 'total_tokens': 23})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(content=\"hi!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "821b1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c2e5b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ciao!'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse the AIMessage object to just extract the content text\n",
    "result = model.invoke(messages)\n",
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e5a101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...or, we can \"chain\" the operations together, using | symbol\n",
    "# (option + shift + L to get pipe on azerty keyboard)\n",
    "\n",
    "chain = model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09a8cfb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ciao!'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b88bbfa",
   "metadata": {},
   "source": [
    "## Use Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef4d9cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5aed3b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create a string that we will format to be the system message\n",
    "system_template = \"Translate the following into {language}:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71ad0dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we can create the PromptTemplate. This will be a combination of the system_template \n",
    "# as well as a simpler template for where the put the text.\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f27c36bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following into french:'), HumanMessage(content='hi')])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The input to this prompt template is a dictionary.\n",
    "# We can play around with this prompt template by itself to see what it does by itself.\n",
    "result = prompt_template.invoke({\"language\": \"french\", \"text\": \"hi\"})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "247805fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following into french:'),\n",
       " HumanMessage(content='hi')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see that it returns a ChatPromptValue that consists of two messages.\n",
    "# If we want to access the messages directly we do:\n",
    "result.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7671d82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好 (nǐ hǎo)'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can chain the components together as follows:\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "chain.invoke({\n",
    "    \"language\": \"mandarin\",\n",
    "    \"text\": \"hi\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b048075c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
